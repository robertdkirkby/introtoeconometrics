{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: extra token \"IPython\" after end of expression",
     "output_type": "error",
     "traceback": [
      "syntax: extra token \"IPython\" after end of expression",
      ""
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: extra token \"as\" after end of expression",
     "output_type": "error",
     "traceback": [
      "syntax: extra token \"as\" after end of expression",
      ""
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.patches as patches\n",
    "#plt.style.reload_library()\n",
    "#plt.style.use(\"ggplot\")\n",
    "from IPython.display import (\n",
    "    display_pretty, display_html, display_jpeg,\n",
    "    display_png, display_json, display_latex, display_svg\n",
    ")\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#from pandas_datareader import data, wb\n",
    "#from scipy import linalg, optimize\n",
    "#from datetime import datetime, date, timedelta\n",
    "from IPython.display import Audio\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Latex\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from IPython.display import SVG\n",
    "\n",
    "import Dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center>QUAN201 - Introduction to Econometrics</center>\n",
    "#### <center>Topic 4: Inference</center>\n",
    "#### <center>2020</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Intro to Econometrics: Inference\n",
    "\n",
    "- Normality assumption\n",
    "- Hypothesis testing: t-test\n",
    "- Hypothesis testing: F-test\n",
    "\n",
    "Reference: [Wooldridge \"Introductory Econometrics - A Modern Approach\"](https://duckduckgo.com/?q=wooldridge+intro+econometrics&t=hk&ia=web), Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inference: basic idea\n",
    "\n",
    "- We can measure $\\hat{\\beta}$ from our sample, e.g., by using OLS regression.\n",
    "- We are realy interested in $\\beta$.\n",
    "- Say we get $\\hat{\\beta}=5$; we know $\\hat{\\beta}$ has a distribution (different samples would give different values).\n",
    "- Can we rule out possibility that $\\beta=6$? Can we rule out possibility that $\\beta=60$?\n",
    "- What can we confidently *infer* about $\\beta$ based on what we find for $\\hat{\\beta}$ in our sample?\n",
    "- More generally: What can we *infer* about the population, based on our sample?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does our OLS estimator tell us about the DGP?\n",
    "\n",
    "- Recall the “I love numbers”-course experiments of not-my brother?\n",
    "<center><img src='Material/Topic4_Fig1.png' width='400' height='40'/></center>\n",
    "\n",
    "- The first OLS estimate suggested that each course increases the math score by 0.15 points ($\\hat{\\beta}_1=0.15$)…  \n",
    "<center><img src='Material/Topic4_Fig2.png' width='200' height='40'/></center>\n",
    "\n",
    "- But that was only one estimate….\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does our OLS estimator tell us about the DGP?\n",
    "\n",
    "- For each new random sample we would usually get a different estimate. \n",
    "\n",
    "- So what can we learn about the true value of $\\beta_j$ from $\\hat{\\beta}_j$ in one sample? (We usually only have one sample!)\n",
    "\n",
    "- If MLR 1-4 hold, $\\hat{\\beta}_j$ is our best guess for the true value of $\\beta_j$. But, we want to know:\n",
    "  - Is the effect “real” or a result of chance? \n",
    "  - How close can we expect $\\beta_j$ to be to $\\hat{\\beta}_j$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hypothesis testing, intuition\n",
    "\n",
    "- How would you decide if an estimate is evidence of a ”real” effect (i.e. the true effect is not zero) and not a result of chance? \n",
    "\n",
    "- Recall that we can estimate the variance of an OLS estimator.\n",
    "\n",
    "- An estimate is more likely the result of a ”real” effect if:\n",
    "  - The estimated value is large in absolute terms.\n",
    "  - The variance of the estimator is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hypothesis testing, outline\n",
    "\n",
    "- We sometimes want to know whether an effect is ”real” or just a result of chance ($\\beta_j = 0$). \n",
    "  - Null hypothesis (H0): $\\beta_j = 0$ (there is no effect, our $\\hat{\\beta}_j$ is due to chance)\n",
    "  - Alternative hypothesis (H1): $\\beta_j \\neq 0$ (the effect is ”real”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap Normal Distribution\n",
    "\n",
    "$X$ is normally distributed with mean $\\mu$ and standard devidation $\\sigma$\n",
    "\n",
    "<center><img src='Material/Topic4_Fig3.png' width='500' height='80'/></center>\n",
    "\n",
    "What are the properties of the normal distribution? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We can standardize a normal distribution by $\\frac{x-\\mu}{\\sigma} \\to N(0,1)$\n",
    "<center><img src='Material/Topic4_Fig4.png' width='600' height='160'/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem\n",
    "\n",
    "- Our $\\hat{\\beta}_j$ is drawn from a distribution.\n",
    "- The standard error of $\\hat{\\beta}_j$ tells us how wide this distribution is.\n",
    "- We don’t know the shape of the distribution of $\\hat{\\beta}_j$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Different distributions\n",
    "\n",
    "<center><img src='Material/Topic4_Fig5.png' width='600' height='160'/></center>\n",
    "\n",
    "[Advanced technical footnote: any function that is non-negative and integrates (adds up to) 1 is the pdf of a distribution.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distribution of $\\hat{\\beta}_j$\n",
    "\n",
    "- $\\hat{\\beta}_j$ is normally distributed if:  \n",
    "  - the error term $u$ is normally distributed\n",
    "\n",
    "- $\\hat{\\beta}_j$ is approximately normally distributed if:\n",
    "  - we have large samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Empirical Distribution of  $\\hat{\\beta}_j$\n",
    "\n",
    "<center><img src='Material/Topic4_Fig6.png' width='600' height='160'/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hypothesis testing, short\n",
    "\n",
    "- The means and standard deviations of $\\hat{\\beta}_j$ depend on the context.\n",
    "  - If $\\hat{\\beta}_j$ is unbiased, the mean is equal to $\\beta_j$\n",
    "  - The standard deviation also depends, among other things, on the sample size (see formula for variance of $\\hat{\\beta}_j$) \n",
    "<center><img src='Material/Topic4_Fig7.png' width='400' height='100'/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hypothesis testing, short\n",
    "\n",
    "- We therefore have to standardize: Under MLR 1-6 one can show that:\n",
    "  - The ratio of $\\hat{\\beta}_j - \\beta_j$ to the standard deviation follows a standard normal distribution. \n",
    "<center><img src='Material/Topic4_Fig8.png' width='400' height='60'/></center>\n",
    "\n",
    "  - The ratio of $\\hat{\\beta}_j - \\beta_j$ to the standard error (called t-statistic) follows a t-distribution.\n",
    "<center><img src='Material/Topic4_Fig9.png' width='400' height='60'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The normal distribution\n",
    "\n",
    "<center><img src='Material/Topic4_Fig10.png' width='600' height='120'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The t distribution\n",
    "\n",
    "<center><img src='Material/Topic4_Fig11.png' width='600' height='120'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hypothesis Testing, short\n",
    "\n",
    "We can then ask: If the hypothesized value of $\\beta_j$ is true (usually $\\beta_j=0$), what is the probability of observing a t-statistic as extreme as the one we have?\n",
    "\n",
    "Each value of the t-statistic is associated with a specific probability: If this probability is low, we conclude that the hypothesized value is probably *not* true.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hypothesis Testing, short\n",
    "\n",
    "<center><img src='Material/Topic4_Fig12.png' width='600' height='120'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: Hypothesis Testing\n",
    "\n",
    "Goal: find out whether $\\beta_j$ is different from zero? \n",
    "\n",
    "Remember that $\\hat{\\beta}_j$ has a distribution? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Empirical Distribution of $\\hat{\\beta}_j$\n",
    "\n",
    "<center><img src='Material/Topic4_Fig6.png' width='600' height='120'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We can standardize a normal distribution by $\\frac{x-\\mu}{\\sigma} \\to N(0,1)$\n",
    "<center><img src='Material/Topic4_Fig4.png' width='600' height='160'/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hypothesis Testing, short\n",
    "\n",
    "- We therefore have to standardize: Under MLR 1-6 one can show that:\n",
    "  - The ratio of $\\hat{\\beta}_j - \\beta_j$ to the standard deviation follows a standard normal distribution. \n",
    "<center><img src='Material/Topic4_Fig8.png' width='400' height='60'/></center>\n",
    "\n",
    "  - The ratio of $\\hat{\\beta}_j - \\beta_j$ to the standard error (called t-statistic) follows a t-distribution.\n",
    "<center><img src='Material/Topic4_Fig9.png' width='400' height='60'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "We assume that null hypothesis is true (typically $\\beta_j = 0$)\n",
    "\n",
    "Calculate t-statistic: $\\frac{\\hat{\\beta}_j-0}{se(\\hat{\\beta}_j)}$\n",
    "\n",
    "<center><img src='Material/Topic4_Fig13.png' width='150' height='60'/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hypothesis Testing, short\n",
    "\n",
    "<center><img src='Material/Topic4_Fig12.png' width='600' height='120'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hypothesis testing, step by step\n",
    "\n",
    "We continue to make assumptions MLR.1-5 introduced previously.\n",
    "\n",
    "We *additionally* assume that the unobserved error term ($u$) is **normally distributed** in the population.\n",
    "\n",
    "<center><img src='Material/Topic4_Fig14.png' width='600' height='80'/></center>\n",
    "\n",
    "This is often referred to as the **normality assumption**. (Note that non-normality of $u$ is not a problem if we have large samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Under the *classical linear model (CLM) assumptions* (MLR 1. -6.), the error term u is normally distributed with constant variance (the latter due to homoskedasticity):\n",
    "\n",
    "<center><img src='Material/Topic4_Fig15.png' width='600' height='160'/></center>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### But why are we assuming normality?\n",
    "\n",
    "Answer: It implies that the OLS estimator $\\hat{\\beta}_j$ follows a normal distribution too.   \n",
    "**Theorem 4.1**: Under the CLM assumptions (MLR.1-6):\n",
    "$$\\hat{\\beta}_j \\sim Normal(\\beta_j,Var(\\hat{\\beta}_j))$$\n",
    "\n",
    "where\n",
    "$$ Var(\\hat{\\beta}_j) = \\frac{\\sigma^2}{SST_j (1-R^2_j)} $$\n",
    "\n",
    "[Do you remember how the R-squared is defined? Why does it have a j-subscript? If you’re not sure, check the the slides for topic 3.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The result that $\\hat{\\beta}_j \\sim Normal(\\beta_j, Var(\\hat{\\beta}_j))$  \n",
    "**implies** that\n",
    "<center><img src='Material/Topic4_Fig16.png' width='450' height='100'/></center>\n",
    "\n",
    "where\n",
    "$$ sd(\\hat{\\beta}_j)= \\sqrt{Var(\\hat{\\beta}_j)}=\\frac{\\sigma}{\\sqrt{SST_j (1-R_j^2)}} $$\n",
    "\n",
    "*In words*: the **difference** between the estimated value and the true parameter value, divided by the standard deviation of the estimator, is normally distributed with mean 0 and standard deviation equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $$\\frac{\\hat{\\beta}_j - \\beta_j}{sd(\\hat{\\beta}_j)} \\sim Normal(0, 1)$$\n",
    "\n",
    "Note, we don’t observe $sd(\\hat{\\beta}_j)$, because it depends on $\\sigma$ (the standard deviation of the error term $u$), which is an unknown parameter. \n",
    "\n",
    "But we can calculated the standard error $se(\\hat{\\beta}_j)$ which is an estimate of $sd(\\hat{\\beta}_j)$. \n",
    "\n",
    "If we replace $sd(\\hat{\\beta}_j)$ with $se(\\hat{\\beta}_j)$ we get the t-statistic, which follows a t-distribution instead of a normal distribution. \n",
    "\n",
    "The test is therefore often referred to as a t-test.\n",
    "\n",
    "<sub><sup>[Technical footnote: $sd(\\hat{\\beta}_j)$ is a number. The normal distribution minus a number divided by a number gives us a (different) normal distribution. $se(\\hat{\\beta}_j)$ in contrast has a distribution (namely it follows a chi-squared distribution) hence why we get something different.]</sub></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Standard error of  $\\hat{\\beta}_j$\n",
    "\n",
    "Formula for standard error:\n",
    "$$ se(\\hat{\\beta}_j)= \\sqrt{Var(\\hat{\\beta}_j)}=\\frac{\\hat{\\sigma}}{\\sqrt{SST_j (1-R_j^2)}} $$\n",
    "\n",
    "Where $\\hat{\\sigma}$ is based on the OLS residuals,\n",
    "$$ \\hat{\\sigma}^2 = \\frac{SSR}{n-k-1}  = \\frac{\\sum_{i=1}^n (\\hat{u}_i)^2 }{n-k-1} $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## t-statistic\n",
    "\n",
    "The t statistic or the t ratio of $\\hat{\\beta}_j$ is defined as:\n",
    "\n",
    "### The standard case (If $H_0: \\beta_j=0$): $t_{\\hat{\\beta}_j} = \\frac{\\hat{\\beta}_j}{se(\\hat{\\beta}_j)}$\n",
    "\n",
    "[General case (If $H_0: \\beta_j=a_j$): $t = \\frac{\\hat{\\beta}_j-a_j}{se(\\hat{\\beta}_j)}$ ]\n",
    "\n",
    "- For the standard case, the t-statistic is easy to compute: just divide your coefficient estimate by the standard error.\n",
    "- Stata will do this for you.\n",
    "- Since the se is always positive, the t-statistic always has the same sign as the coefficient estimate.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### t-statistics in Stata\n",
    "\n",
    "<center><img src='Material/Topic4_Fig17.png' width='600' height='80'/></center>\n",
    "\n",
    "Quiz: Calculate the t-statistic for skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### t distribution for the Standardized Estimators\n",
    "\n",
    "under the CLM assumptions:\n",
    "\n",
    "$$\\frac{\\hat{\\beta}_j - \\beta_j}{se(\\hat{\\beta}_j)} \\sim t_{n-k-1}$$\n",
    "\n",
    "where $k+1$ is the number of unknown parameters in the population model ($k$ slope parameters & the intercept).\n",
    "\n",
    "In words, this says that the **difference** between the estimated value and the true parameter value, divided by the **standard error** of the estimator follows a *t-distribution with $n-k-1$ **degrees of freedom** *."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shape of the t-distribution\n",
    "\n",
    "<center><img src='Material/Topic4_Fig18.png' width='400' height='80'/></center>\n",
    "\n",
    "- The shape is similar to the standard normal distribution - but more spread out and more area in the tails.\n",
    "- As the degrees of freedom increases (e.g. when n increases), the t distribution approaches the standard normal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Testing hypotheses about a single parameter of the DGP: The t test\n",
    "\n",
    "Our starting point is the model (DGP):\n",
    "$$ y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k +u $$\n",
    "\n",
    "Our goal is to test hypotheses about a particular $\\beta_j$\n",
    "\n",
    "*Remember*: $\\beta_j$ is an **unknown parameter** and we will never know its value with certainty. But we can **hypothesize** about the value of $\\beta_j$ and then use statistical inference to *test* our hypothesis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hypothesis testing, step by step for two sided alternatives (standard case)\n",
    "\n",
    "Step $1$. Specify null hypothesis (*H0*) and alternative Hypothesis (*H1*).  \n",
    "*H0*: $\\beta_j = 0$  \n",
    "*H1*: $\\beta_j \\neq 0$ (two sided alternative)\n",
    "\n",
    "Step $2$. Decide on a **significance level** ($\\alpha$): highest probability we are willing to accept of rejecting *H0* if it is in fact true.  \n",
    "The most common significance level is 5%.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hypothesis testing, step by step\n",
    "  \n",
    "Step $3$. Stata computes the t-statistic and looks up the **p-value** associated with it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example: \n",
    "\n",
    "Suppose t = 1.85 and df=40. \n",
    "\n",
    "This results in a p-value = 0.0718.\n",
    "\n",
    "<center><img src='Material/Topic4_Fig19.png' width='400' height='80'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### *p*-values in Stata\n",
    "\n",
    "<center><img src='Material/Topic4_Fig20.png' width='600' height='120'/></center>\n",
    "\n",
    "Interpretation: The p-value is the probability of observing a t-statistic as extreme as we did **if the null hypothesis** is **true**.\n",
    "\n",
    "Thus, **small** p-values are evidence **against** the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hypothesis testing, step by step\n",
    "\n",
    "Step $4$. Compare significance level ($\\alpha$) with p-value. Decision rule:  \n",
    "If $p-value>\\alpha, \\quad$ H0 is not rejected.  \n",
    "If $p-value \\leq \\alpha, \\quad$ H0 is rejected in favor of H1.\n",
    "\n",
    "<center><img src='Material/Topic4_Fig21.png' width='350' height='60'/></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rule of thumb\n",
    "\n",
    "If you can’t look up the p-value, this simple rule of thumb can help:\n",
    "- For two sided alternatives, 5% significance level and df > 60:\n",
    "- t-statistic lower than -2 or higher than 2 implies statistical significance. (i.e. We can reject the null hypothesis.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practice rule of thumb\n",
    "\n",
    "<center><img src='Material/Topic4_Fig22.png' width='600' height='80'/></center>\n",
    "\n",
    "Given our rule of thumb, what can we say about the statistical significance at the 5% level of ACT and skipped?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: effect of experience on wage\n",
    "\n",
    "$$ wage = \\beta_0 + \\beta_1 educ + \\beta_2 exper + u $$\n",
    "\n",
    "The null hypothesis $H0: \\beta_2=0$, years of experience has no effect on wage.\n",
    "\n",
    "H0: $\\beta_2=0$  \n",
    "H1: $\\beta_2 \\neq 0$\n",
    "\n",
    "Significance level: 5%\n",
    "\n",
    "Assume that CLM hold.\n",
    "\n",
    "(continues next slide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='Material/Topic4_Fig23.png' width='600' height='120'/></center>\n",
    "\n",
    "*(answer next slide)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='Material/Topic4_Fig23.png' width='600' height='120'/></center>\n",
    "\n",
    "t-statistic: 6.39, p-value: 0.000\n",
    "\n",
    "Significance level > p-value $\\to$ reject H0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Testing other hypotheses about $\\beta_j$\n",
    "\n",
    "Although *H0*: $\\beta_j=0$ is the most common hypothesis, we sometimes want to test whether $\\beta_j$ is equal to some other given constant. Suppose the null hypothesis is\n",
    "$$ H0: \\, \\beta_j=a_j$$\n",
    "\n",
    "In this case the appropriate t-statistic is:\n",
    "$$t = \\frac{\\hat{\\beta}_j-a_j}{se(\\hat{\\beta}_j)}$$\n",
    "\n",
    "The rest of the t-test is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question for Class\n",
    "\n",
    "Is the *hsGPA* coefficient in the regression below significantly different from **1** (against a two-sided alternative) at the 5% level?\n",
    "\n",
    "<center><img src='Material/Topic4_Fig24.png' width='600' height='120'/></center>\n",
    "\n",
    "A) Yes              B) No            C) We can’t say"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.3 Confidence intervals\n",
    "\n",
    "Once we have estimated the DGP parameters $\\beta_0$,$\\beta_1$,...,$\\beta_k$, and obtained the associated standard errors, we can easily construct a confidence interval (CI) for each $\\beta_j$.\n",
    "\n",
    "The CI provides a range of likely values for the unknown $\\beta_j$.\n",
    "\n",
    "Recall that $\\frac{\\hat{\\beta}_j-\\beta_j}{se(\\hat{\\beta}_j)}$ has a t distribution with $n-k-1$ degrees of freedom (df).\n",
    "\n",
    "Define a 95% confidence interval for $\\beta_j$ as \n",
    "$$ \\hat{\\beta}_j \\pm c \\cdot se(\\hat{\\beta}_j) $$\n",
    "where constant $c$ the $97.5^{th}$ percentile in the $t_{n-k-1}$ distribution.\n",
    "\n",
    "<sub><sup>Question: why 97.5, not 95?</sub></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Critical Value\n",
    "\n",
    "For a 95% Confidence-Interval, the critical value c is chosen to make the area in **each** tail equal 2.5%, i.e., c is the 97.5th percentile in the t distribution. \n",
    "\n",
    "The graph shows that, if df=26, then c=2.06.\n",
    "<center><img src='Material/Topic4_Fig25.png' width='400' height='100'/></center>\n",
    "\n",
    "If *H0* were true, we would expect a t-statistic larger than 2.06 or smaller than -2.06 in only 5% of the cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confidence intervals (1)\n",
    "\n",
    "$$ \\hat{\\beta}_j \\pm c \\cdot se(\\hat{\\beta}_j) \\left\\{ \\begin{align*} \\bar{\\beta}_j = \\hat{\\beta}_j + c \\cdot se(\\hat{\\beta}_j) \\quad \\text{upper limit} \\\\ \\underline{\\beta}_j = \\hat{\\beta}_j - c \\cdot se(\\hat{\\beta}_j)  \\quad \\text{lower limit} \\end{align*} \\right. $$\n",
    "\n",
    "Constructing a **95% confidence interval** involves calculating two values, $\\bar{\\beta}_j$ and $\\underline{\\beta}_j$, which are such that if random samples were obtained many times, with the confidence interval ($\\bar{\\beta}_j$, $\\underline{\\beta}_j$) computed each time, then 95% of these intervals would contain the unknown. \n",
    "\n",
    "This implies that if our 95% confidence interval does not include zero, we can reject H0 at the 5% level.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confidence intervals (2)\n",
    "\n",
    "$$ \\hat{\\beta}_j \\pm c \\cdot se(\\hat{\\beta}_j) \\left\\{ \\begin{align*} \\bar{\\beta}_j = \\hat{\\beta}_j + c \\cdot se(\\hat{\\beta}_j) \\quad \\text{upper limit} \\\\ \\underline{\\beta}_j = \\hat{\\beta}_j - c \\cdot se(\\hat{\\beta}_j)  \\quad \\text{lower limit} \\end{align*} \\right. $$\n",
    "\n",
    "Unfortunately, for the *single* sample that we use to construct the CI (confidence interval), we do not know whether $\\beta_j$ is actually contained in the interval.\n",
    "\n",
    "We believe we have obtained a sample that is one of the 95% of all samples where the CI contains $\\beta_j$, but we have no guarantee.\n",
    "\n",
    "Question: What happens to the CI when se() increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Confidence Intervals in Stata\n",
    "\n",
    "Assume that CLM assumptions hold.\n",
    "<center><img src='Material/Topic4_Fig26.png' width='600' height='120'/></center>\n",
    "\n",
    "Question: Which coefficient are statistically significant at the 5% level?   \n",
    "Use the Use the 95% CI in your to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Do baseball players get paid according to their performance?\n",
    "\n",
    "- What do you think?\n",
    "- How is it in other sports?\n",
    "<center><img src='Material/Topic4_Fig27.png' width='400' height='60'/></center>\n",
    "\n",
    "- Let’s find out! We have data on salary and performance statistics. \n",
    "\n",
    "[Personal thoughts: Baseball is a contender for the worst sport ever invented. This is clear from the fact that the most exciting thing in the game is when the ball is no longer on the field. :P]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Baseball, Model and Data\n",
    "\n",
    "Consider the following model of (major league) baseball players’ salaries:\n",
    "$$ log(salary) = \\beta_0 + \\beta_1 years + \\beta_2 gamesyr + \\beta_3 bavg + \\beta_4 hrunsyr + \\beta_5 rbisyr + u$$\n",
    "\n",
    "Variables:\n",
    "-  $salary$ = total 1993 salary;\n",
    "- $years$ = years in the league;\n",
    "- $gamesyr$ = average games played per year;\n",
    "- $\\color{red}{bavg}$ = career batting average; \n",
    "- $\\color{red}{hrunsyr}$ = home runs per year; \n",
    "- $\\color{red}{rbisyr}$ = runs batted per year\n",
    "\n",
    "[We will consider the last three variables in red as those capturing 'performance'.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### OLS estimates\n",
    "\n",
    "Assume that CLM assumptions hold.\n",
    "<center><img src='Material/Topic4_Fig28.png' width='600' height='120'/></center>\n",
    "\n",
    "- So each of the coefficients is statistically insignificiant. \n",
    "- Does that imply we should not reject H0?\n",
    "\n",
    "Note: Because the dependent variable is *log* salary, we interpret the coefficients as percentage change in salary. For example, the model predicts that a one year increase in the league increases salary by 6.8%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### What is our hypothesis?\n",
    "\n",
    "$$ log(salary) = \\beta_0 + \\beta_1 years + \\beta_2 gamesyr + \\beta_3 bavg + \\beta_4 hrunsyr + \\beta_5 rbisyr + u$$\n",
    "\n",
    "Our hypothesis is very general:  \n",
    "$H0: \\beta_3=0, \\, \\beta_4=0, \\, \\beta_4=0$  \n",
    "$H1:$ *H0 is not true*\n",
    "\n",
    "\n",
    "In economics we sometimes want to test whether a number of coefficients are jointly significant. We can do this with an F-test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### F-test, intuition\n",
    "\n",
    "- *Goal*: test whether a **group** of variables has no effect on the dependent variable. \n",
    "- Remember the discussion of $R^2$ statistic:\n",
    "  - We can express y in terms of explained and unexplained part: $y_i = \\hat{y}_i+u_i$\n",
    "  - The total variation in y is equal to the sum of explained and unexplained variation: \n",
    "  $$ SST=SSE+SSR $$\n",
    "  \n",
    "$ \\text{Sum of Squares Total: }SST= \\sum_{i=1}^n (y_i-\\bar{y})^2 $  \n",
    "$ \\text{Sum of Squares Explained: }SSE= \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2 $  \n",
    "$ \\text{Sum of Squares Residual: }SSR= \\sum_{i=1}^n (\\hat{u}_i)^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### F-test, intuition\n",
    "\n",
    "- What happens when our model becomes better at explaining the variation in $y$?\n",
    "  - SSE increases and SSR decreases\n",
    "- Remember, $R^2$ always increases when we include more variables. \n",
    "- The F-test tells us if the reduction in SSR from adding *a group of* variables is large enough that we can reject $H0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## F-test, step by step\n",
    "\n",
    "We compare the SSR of the restricted model to the SSR of the unrestricted model.\n",
    "\n",
    "**Unrestricted** model:\n",
    "$$ log(salary) = \\beta_0 + \\beta_1 years + \\beta_2 gamesyr + \\beta_3 bavg + \\beta_4 hrunsyr + \\beta_5 rbisyr + u$$\n",
    "\n",
    "**Restricted** model:\n",
    "$$ log(salary) = \\beta_0 + \\beta_1 years + \\beta_2 gamesyr + u$$\n",
    "\n",
    "Exclusion restrictions: $H0: \\beta_3=0, \\, \\beta_4=0, \\, \\beta_4=0$\n",
    "\n",
    "Econometrics jargon: ”impose restrictions” = other values (in our case zeros) are assumed for certain parameters than those obtained when the model is estimated freely. The null hypothesis is that these parameters are \"jointly\" (all) zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question for Class\n",
    "\n",
    "What can we say about the relationship between SSRr and SSRur?\n",
    "\n",
    "a) SSRr ≥ SSRur  \n",
    "b) SSRr ≤ SSRur  \n",
    "c) SSRr = SSRur  \n",
    "d) it depends / we can’t say\n",
    "\n",
    "[SSRr=SSR of restricted model, SSRur=SSR of unrestricted model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### F-test: key question\n",
    "\n",
    "When we add the variables to the model and move from the restricted to the unrestricted model...\n",
    "\n",
    "*Key question*: Does SSR *decrease enough* for it to be warranted to reject the null hypothesis?\n",
    "\n",
    "How much should the SSR *decrease* so that it is likely not a result of chance, i.e., statistically significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The F statistic\n",
    "\n",
    "<center><img src='Material/Topic4_Fig29.png' width='450' height='100'/></center>\n",
    "\n",
    "- SSRr is the sum of squared residuals for the restricted model; \n",
    "- SSRur is the SSR for the unrestricted model.\n",
    "- $q$ is the number of restrictions imposed in moving from the unrestricted to the restricted model.\n",
    "- $n-k-1$ = degrees of freedom.\n",
    "- $n$ is the number of obervations.\n",
    "- $k$ is the number of variables (in the unrestricted model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The F statistic & the F distribution\n",
    "\n",
    "To use the F statistic we must know its sampling distribution under the null (this enables us to choose critical values & rejection rules).\n",
    "\n",
    "Under $H0$, and when the CLM assumptions hold, F follows an F distribution with (q,n-k-1) degrees of freedom: $$F \\sim F_{q,n-k-1}$$\n",
    "\n",
    "The critical values for significance level of 5% for the F distribution are given in Table G.3.b.\n",
    "\n",
    "Rejection rule: Reject H0 in favor of H1 at (say) the 5% significance level if F>c, where c is the 95th percentile in the $F_{q,n-k-1}$ distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Unrestricted Model\n",
    "<center><img src='Material/Topic4_Fig31.png' width='350' height='80'/></center>\n",
    "\n",
    "#### Restricted Model\n",
    "<center><img src='Material/Topic4_Fig32.png' width='350' height='80'/></center>\n",
    "\n",
    "$$ F=\\frac{(198.311- 183.186)/3}{183.186/347}=9.55 $$\n",
    "\n",
    "#### Implication?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Carrying out an F test is easy in Stata…\n",
    "\n",
    "\n",
    "<center><img src='Material/Topic4_Fig33.png' width='600' height='160'/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### If we drop $rbisyr$...\n",
    "\n",
    "<center><img src='Material/Topic4_Fig34.png' width='400' height='120'/></center>\n",
    "\n",
    "Recall from topic 3: $se(\\hat{\\beta}_j) = \\frac{\\hat{\\sigma}}{\\sqrt{SST_j (1-R_j^2)}}$\n",
    "\n",
    "This example shows quite clearly that by including near-multicollinear regressors you will get large standard errors and, consequently, low t-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computing p-values for F tests\n",
    "\n",
    "The p-value is defined as\n",
    "$$ p-value = Pr(\\textbf{F}>F)$$\n",
    "\n",
    "where **F** is the random variable with $df=(q,n-k-1)$ and $F$ is the actual value of the test statistic.\n",
    "\n",
    "Interpretation of *p-value*: The probability of observing a value for F as large as we did *given* that the null hypothesis is true.\n",
    "\n",
    "For example, $p-value = 0.016$ implies such probability is only 1.6% - hence we would reject the null hypothesis at the 5% level (but not at the 1% level)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The F statistic for overall significance of a regression\n",
    "\n",
    "Consider the following model and null hypothesis:\n",
    "$$ y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k +u $$\n",
    "\n",
    "H0: $x_1, x_2,..., x_k$ do not help to explain $y$  \n",
    "That is,  \n",
    "$H0: \\beta_1=\\beta_2=...=\\beta_k=0$\n",
    "\n",
    "Model under $H0$: $y=\\beta_0 + u$\n",
    "\n",
    "It can be shown that, in this case, the F statistic can be computed as\n",
    "$$ F=\\frac{R^2/k}{(1-R^2)/(n-k-1)}$$\n",
    "\n",
    "[Note: This last formula is only valid if you want to test whether **all explanatory variables** are jointly significant. (Thought: If you used the earlier formula, what is $q$ here?)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='Material/Topic4_Fig35.png' width='500' height='120'/></center>\n",
    "\n",
    "$$ F=\\frac{R^2/k}{(1-R^2)/(n-k-1)}=\\frac{0.628/5}{(1-0.628)/347}=117$$\n",
    "\n",
    "- This type of test determines the **overall significance of the regression**.\n",
    "- If we fail to reject the null hypothesis, our model has very little explanatory power – it is not a statistically significant improvement over a model with no explanatory variables!\n",
    "- In such a case, we should probably look for other explanatory variables..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hypothesis testing, special case: one sided alternative\n",
    "\n",
    "In some cases, the alternative Hypothesis is only one sided. \n",
    "\n",
    "H0: $\\beta_j=0$  \n",
    "H1: $\\beta_j>0$ or H1: $\\beta_j<0$\n",
    "\n",
    "If H1: $\\beta_j=0$ this means that we believe that the effect of $x_j$ can only be positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hypothesis testing: one sided alternative\n",
    "\n",
    "The p-values reported in Stata are for two-sided tests (i.e. H0: $\\beta_j=0$, and H1: $\\beta_j\\neq 0$)\n",
    "\n",
    "To get the p-value for a one sided test, we just divide the p-value reported in Stata by 2. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Standard Case: Two sided alternative\n",
    "\n",
    "$$\\text{H0: }\\beta_j=0\\text{, and H1: } \\beta_j\\neq 0$$\n",
    "\n",
    "<center><img src='Material/Topic4_Fig36.png' width='400' height='80'/></center>\n",
    "\n",
    "Significance level = 5% $\\to$ We accept being wrong in 5% of the cases.  \n",
    "Extreme t-statistics (both positive and negative) are evidence against the null hypothesis.  \n",
    "Choose critical value for which we would be wrong in 5% of the cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Special Case: One sided alternative\n",
    "\n",
    "$$\\text{H0: }\\beta_j=0\\text{, and H1: } \\beta_j > 0$$\n",
    "\n",
    "<center><img src='Material/Topic4_Fig37.png' width='400' height='80'/></center>\n",
    "\n",
    "Significance level = 5% $\\to$ We accept being wrong in 5% of the cases.  \n",
    "*Large, positive t-statistics are evidence against the null hypothesis.  \n",
    "Large, negative t-statistics are evidence **for** the null hypothesis.*  \n",
    "Choose critical value for which we would be wrong in 5% of the cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One sided is less conservative\n",
    "\n",
    "One sided hypothesis testing less conservative:  \n",
    "- Critical values are smaller\n",
    "- p-values are smaller (1/2 size)\n",
    "\n",
    "This is because we already rule out one direction of the effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hypothesis testing, one sided alternative\n",
    "\n",
    "The key difference is that we look up a different critical value.  \n",
    "Step 1: Specify null and alternative hypothesis (H0: $\\beta_j=0$; H1: $\\beta_j>0$ (or H1:$\\beta_j<0$))  \n",
    "Step 2: Decide on significance level.  \n",
    "Step 3: Compute t-statistic.  \n",
    "Step 4: Divide Stata p-value by 2.   \n",
    "Step 5: Compare p-value (from Step 4) with significance level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 4.1: One sided alternative\n",
    "\n",
    "- To get the p-value for the one sided test, simply divide the p-value reported in Stata by 2\n",
    "<center><img src='Material/Topic4_Fig38.png' width='500' height='100'/></center>\n",
    "\n",
    "- What’s the p-value for the one sided test for the ACT coefficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Don’t forget the CLM assumptions!\n",
    "\n",
    "- The methods for doing inference discussed above (t-values or confidence intervals) will **not** be reliable if the CLM assumptions (MLR.1-6) do not hold. For example:\n",
    "  - **Omitted variables** that correlate with the explanatory variable (violates MLR.4)\n",
    "  - **Heteroskedasticity** (violates MLR.5) \n",
    "  - **Non-normally distributed error terms** (violates MLR.6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Assumption MLR.1*: The model is **linear in parameters**: $y=\\beta_0 + \\beta_1 x_1+...+\\beta_k x_k+u$  \n",
    "*Assumption MLR.2*: **Random sampling**: We have a sample of n observations $\\{x_{i1}, x_{i2},...,x_{ik},y): \\, i=1,2,...,n\\}$ following the population model in Assumption MLR.1  \n",
    "*Assumption MLR.3*: **No perfect collinearity**: In the sample, none of the independent variables is constant and there are no exact linear relationships among the independent variables.  \n",
    "*Assumption MLR.4*: **Zero conditional mean**: The error $u$ has an expected value of zero, given any values of the independent variables: $E[u|x_1,x_2,...,x_k]=0$  \n",
    "*Assumption MLR.5*: **Homoskedasticity**: The error $u$ has the same variance given any value of the explanatory variables.  \n",
    "*Assumption MLR.6*: **Normality**: The population error $u$ is independent of the explanatory variables $x_1,x_2,...,x_k$, and is normally distributed with zero mean and variance $\\sigma^2$: $u \\sim Normal(0, \\sigma^2)$.\n",
    "\n",
    "- It is important to understand what we can conclude if the assumptions hold:\n",
    "  - Under **MLR.1-4**, the OLS estimator is **unbiased**.\n",
    "  - Under **MLR.1-5**, the OLS estimator is the **B**est **L**inear **U**nbiased **E**stimator (**BLUE**).\n",
    "  - **MLR.1-5** are collectively known as the **Gauss-Markov assumptions** (for cross-sectional regression.\n",
    "  - Under **MLR.1-6 (CLM assumptions)**, it is straightforward to do **statistical inference** using conventional OLS standard errors, t statistics and F statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpretation of p-values and t-test\n",
    "\n",
    "- If CLM assumptions hold: We can make inference about the underlying parameters (i.e., we can learn from world 2 about world 1).\n",
    "\n",
    "- If MLR 1.-5 are true, but error term is not normally distributed (MLR 6)\n",
    "  - In small samples $\\to$  estimates are unbiased but p-values (and t and f values) are not correct.\n",
    "  - In large samples $\\to$ p-values are approximately correct, because for large samples the estimator is normaly distributed even if the error is not (see Chapter 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpretation of p-values and t-test\n",
    "\n",
    "- If only MLR 5 is violated \n",
    "  - Estimates are unbiased, but p-values are wrong.  \n",
    "  (We will talk about this case later.)\n",
    "- If MLR 1-4 is violated:\n",
    "  - Estimates are biased. However, p-values can still tell you how likely this estimate is to have occured by chance.  \n",
    "  (Which is still interesting in many cases.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Marriage premium\n",
    "\n",
    "What is the effect of being married on wage?\n",
    "\n",
    "Let’s estimate the following model:\n",
    "$$ wage = \\beta_0 + \\beta_1 educ + \\beta_2 exper + \\color{red}{\\beta_3} married + u $$\n",
    "\n",
    "Where married is a dummy variable which is 1 if the individual is married and 0 otherwise.  \n",
    "(We will talk more about dummy variables later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Before we look at the data...\n",
    "\n",
    "- Do we expect an effect?\n",
    "  - Maybe marriage makes men more productive (married men may work harder, control their temper better, etc.). \n",
    "- How large would such an effect be?\n",
    "- Are there reasons why we believe OLS estimates might be biased? \n",
    "  - Omitted Variable Bias: Maybe married men have other unobserved characteristics (intelligence, skill, etc.) that make them more productive.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='Material/Topic4_Fig39.png' width='600' height='120'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpretation of OLS output\n",
    "\n",
    "- Are MLR 1-4 true? Probably not. \n",
    "- Is MLR 5 true? In practice, we use robust standard errors and don’t worry about this question for now. (We will talk about this later.)\n",
    "- Is the sample large enough? Yes, so we don’t worry about MLR 6.\n",
    "\n",
    "- My interpretation of the results: \n",
    "  - The wage difference between married and not married men conditional on education and experience is statistically significant (i.e., it is most likely not a result of chance). \n",
    "  - Because of likely omitted variable bias we can’t say anything about the causal effect of marriage on wage. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remember how to interpret OLS coefficients.\n",
    "<center><img src='Material/Topic4_Fig40.png' width='600' height='120'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Economic vs. statistical significance\n",
    "\n",
    "- As we have seen, the **statistical significance** of a variable $x_j$ is determined by the size of the coefficient and size of the standard error. \n",
    "- The **economic significance** of a variable is related to the size (and sign) of the estimated *coefficient*.\n",
    "- Too much focus on statistical significance can lead to the false conclusion that a variable is ”important” even though the estimated effect is small.\n",
    "- It is always important to interpret the **magnitude** of the estimated coefficient (in addition to looking at the statistical significance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Guidelines for discussing economic & statistical significance\n",
    "\n",
    "Nice discussion in Section 4.2 in the book. Summary:\n",
    "- Check statistical significance. If significant, discuss the size of the estimated coefficient.\n",
    "- If not significant at (at least) 10% level, check if the sign of the coefficient is in accordance with your expectation.\n",
    "- *There is nothing wrong with getting insignificant results*.\n",
    "- If the sign is the opposite to what you expect **and** the effect is statistically significant, this suggests that something isn’t quite right: perhaps your empirical specification (omitted variables?) or your underlying theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Emerging Best Practice\n",
    "\n",
    "- Focus on *point estimate* and *confidence intervals*.\n",
    "\n",
    "- Why *confidence intervals*? They tell us if 'big' or 'small' values/effects are plausible given the evidence.\n",
    "\n",
    "- Don't focus on statistical significance at a specific cut-off level (95%, 99%, etc.)\n",
    "\n",
    "- Report results for hypothesis tests for statistical significance using p-values.  \n",
    "Big advantage is you can use p-value approach without needing to know the underlying distribution. E.g., we saw p-values in both t-test and F-test.\n",
    "\n",
    "- There are some important exceptions: e.g., in most countries pharmaceutical regulation means drugs can be sold as treating a certain disease if their effect is shown to be statistically significant (non-zero), even if that effect might be small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review/Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hypothesis testing, old school\n",
    "\n",
    "Today, any statistical program reports p-values. \n",
    "\n",
    "In the past, researchers did hypothesis testing by looking up critical values in statistical tables.\n",
    "\n",
    "While this is now hardly done in practice this approach is of some historical importance. \n",
    "\n",
    "You don’t need to do this in the exam. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
