{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: extra token \"IPython\" after end of expression",
     "output_type": "error",
     "traceback": [
      "syntax: extra token \"IPython\" after end of expression",
      ""
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: extra token \"as\" after end of expression",
     "output_type": "error",
     "traceback": [
      "syntax: extra token \"as\" after end of expression",
      ""
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.patches as patches\n",
    "#plt.style.reload_library()\n",
    "#plt.style.use(\"ggplot\")\n",
    "from IPython.display import (\n",
    "    display_pretty, display_html, display_jpeg,\n",
    "    display_png, display_json, display_latex, display_svg\n",
    ")\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#from pandas_datareader import data, wb\n",
    "#from scipy import linalg, optimize\n",
    "#from datetime import datetime, date, timedelta\n",
    "from IPython.display import Audio\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Latex\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from IPython.display import SVG\n",
    "\n",
    "import Dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center>QUAN201 - Introduction to Econometrics</center>\n",
    "#### <center>Topic 3: Multiple OLS</center>\n",
    "#### <center>2020</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quan 201: Multiple Regression Analysis\n",
    "\n",
    "- Motivation\n",
    "- Multiple Regression Model\n",
    "- OLS estimator\n",
    "- Measure of Goodness-of-Fit: R-squared\n",
    "- OLS assumptions\n",
    "- Omitted Variable Bias\n",
    "- Variance of OLS estimator\n",
    "\n",
    "Reference: [Wooldridge \"Introductory Econometrics - A Modern Approach\"](https://duckduckgo.com/?q=wooldridge+intro+econometrics&t=hk&ia=web), Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap\n",
    "\n",
    "- Outcomes of interest are determined by many factors.\n",
    "- We are usually interested in the effect of one particular factor.\n",
    "  - $y=\\beta_0+\\beta_1 X_1 + \\beta_2 X_2 + ...+ \\beta_k X_k$\n",
    "  - $ y=\\beta_0+\\beta_1 X_1 + u $, where $u$ is all other factors.\n",
    "\n",
    "OLS only gives an unbiased estimate of $\\beta$ if $E[u|x] = E[u]$, which is often unrealistic (unless we have data from an experiment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example where E(u|x) = E(u) fails\n",
    "\n",
    "- What is the effect of school expenditures on student test scores? \n",
    "\n",
    "$$ Avg. \\,Test\\, scores =\\beta_0+\\beta_1 avg. \\, expend + \\beta_2 avg. \\, income + u $$\n",
    "\n",
    "- $\\beta_2 \\neq 0$\n",
    "- $Corr(avg. \\, income, avg. \\, expend) \\neq 0$\n",
    "- In the US, schools in are locally funded so suburbs with high average income collect more taxes and can thus have higher expenditures. (By contrast, NZ national government funds all schools.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Regression Analysis\n",
    "\n",
    "- We might have data on some of the other factors that affect the outcome.\n",
    "\n",
    "- **Multiple regression** analysis enables us to include more than one explanatory variable in our model.\n",
    "\n",
    "- By including more variables, we hold these variables explicitly constant. \n",
    "\n",
    "- This brings us closer to the ceteris paribus (all other things being equal) analysis than simple regression. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The model with k independent variables\n",
    "\n",
    "$$y=\\beta_0+\\beta_1 X_1 + \\beta_2 X_2 + ...+ \\beta_k X_k + u$$\n",
    "\n",
    "- $\\beta_0$ is the intercept.\n",
    "- $\\beta_1$ shows the change in $y$ with respect to $x_1$.\n",
    "- $\\beta_2$ shows the change in $y$ with respect to $x_2$.\n",
    "- (etc. for the other $\\beta$ up to $\\beta_k$)\n",
    "- $u$ is the error term. It captures the effect of factors other than $x_1$,$x_2$,...,$x_k$ affecting $y$.\n",
    "- This framework can also be used to generalize the **functional form** (we will talk about this later).\n",
    "\n",
    "- We basically now mention more factors of the DGP explicitly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OLS fitted values and residuals\n",
    "\n",
    "- For observation $i$ the **fitted value** or **predicted value** is simply\n",
    "$$ \\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1 X_1 + \\hat{\\beta}_2 X_2 + ...+ \\hat{\\beta}_k X_k$$\n",
    "\n",
    "- The residual for observation $i$ is defined just as in the simple regression case:\n",
    "$$ \\hat{u}_i = y_i - \\hat{y}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question for Class\n",
    "\n",
    "- Consider the following OLS estimates:\n",
    "$$\\widehat{testscore}=\\underbrace{88}_{\\hat{\\beta}_0} + \\underbrace{1}_{\\hat{\\beta}_1}*classsize + \\underbrace{6}_{\\hat{\\beta}_2}*ability$$\n",
    "\n",
    "Consider Amanda: \n",
    "- Testscore = 100\n",
    "- Class size = 20\n",
    "- Ability = 2\n",
    "\n",
    "- What is Amanda’s *predicted* testscore?\n",
    "- What is Amanda’s residual?\n",
    "- What is Amanda’s error (u)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Obtaining OLS estimates\n",
    "\n",
    "OLS estimates:\n",
    "$$ \\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1 X_1 + \\hat{\\beta}_2 X_2 + ...+ \\hat{\\beta}_k X_k$$\n",
    "\n",
    "OLS chooses $\\hat{\\beta}_0$, $\\hat{\\beta}_1$, $\\hat{\\beta}_2$,...,$\\hat{\\beta}_k$\n",
    "to minimise the sum of squared residuals.\n",
    "\n",
    "$$ \\sum_{i=1}^n \\hat{u}_i^2 = \\sum_{i=1}^n (y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2}- ...- \\hat{\\beta}_k x_{ik}) ^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: The FOC for $\\hat{\\beta}_1$\n",
    "\n",
    "The minimization problem: Choose $\\hat{\\beta}$ so as to minimize: \n",
    "$$ \\sum_{i=1}^n (y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2}- ...- \\hat{\\beta}_k x_{ik}) ^2$$\n",
    "One part of this will be taking the partial derivative w.r.t. $\\hat{\\beta}_1$ and setting that equal to zero, this gives the FOC (first order condition) for $\\hat{\\beta}_1$. Namely,\n",
    "\n",
    "$$ \\sum_{i=1}^n x_{i1}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2}- ...- \\hat{\\beta}_k x_{ik})=0$$\n",
    "\n",
    "[Technical note: there is a 2 that multiplies this, but we can simplify be cancelling it.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Full FOCs: k+1 equations for k+1 unknowns\n",
    "\n",
    "Taking the FOCs for each element of $\\hat{\\beta}$ we get the system of equations:\n",
    "\n",
    "$$ \\sum_{i=1}^n (y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2}- ...- \\hat{\\beta}_k x_{ik})=0$$\n",
    "\n",
    "$$ \\sum_{i=1}^n x_{i1}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2}- ...- \\hat{\\beta}_k x_{ik})=0$$\n",
    "\n",
    "$$ \\sum_{i=1}^n x_{i2}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2}- ...- \\hat{\\beta}_k x_{ik})=0$$\n",
    "\n",
    "$$ \\vdots $$\n",
    "\n",
    "$$ \\sum_{i=1}^n x_{ik}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2}- ...- \\hat{\\beta}_k x_{ik})=0$$\n",
    "\n",
    "Solving these is simple, but tedious. (Much easier using matrices and vectors.)\n",
    "\n",
    "We will never actually do this. Computer calculates the solution for us.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sample properties of OLS estimator\n",
    "\n",
    "$$ \\sum_{i=1}^n (y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2}- ...- \\hat{\\beta}_k x_{ik})=0$$\n",
    "\n",
    "$$ \\sum_{i=1}^n x_{i1}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2}- ...- \\hat{\\beta}_k x_{ik})=0$$\n",
    "\n",
    "$$ \\sum_{i=1}^n x_{i2}(\\underbrace{y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2}- ...- \\hat{\\beta}_k x_{ik}}_{residual, \\, \\hat{u}_i})=0$$\n",
    "\n",
    "$$ \\vdots $$\n",
    "\n",
    "$$ \\sum_{i=1}^n x_{ik}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2}- ...- \\hat{\\beta}_k x_{ik})=0$$\n",
    "\n",
    "From the FOCs it follows that:\n",
    "- Average value and sum of $\\hat{u}_i$'s are equal to zero.\n",
    "- Covariance of each variable, e.g. $x_3$, with $\\hat{u}$ is equal to zero.\n",
    "\n",
    "Note: this is mechanically so because of the way OLS is estimated and does not say anything about the unobserved error term $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretation of equation with k independent variables\n",
    "\n",
    "The coefficient on $x_1$ ($\\hat{\\beta}_1$) measures the change in $\\hat{y}$ due to a one-unit increase in $x_1$, holding all other independent variables fixed:\n",
    "$$ \\Delta \\hat{y} = \\hat{\\beta}_1 \\Delta x_1 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Partialling out interpretation of $\\hat{\\beta}_j$\n",
    "\n",
    "$$\\hat{\\beta}_j=\\frac{Cov(y,\\tilde{x}_j)}{Var(\\tilde{x}_j)}$$\n",
    "Where $\\tilde{x}_j$ are the residuals of a regression of $x_j$ on all the covariates ($x_1$,...$x_{j-1}$,$x_{j+1}$,...,$x_k$)\n",
    "\n",
    "- $\\hat{\\beta}_j$ measures the sample relationship between $y$ and $x_j$ after all the other covariates have been partialled out. \n",
    "\n",
    "(Also use wording '*after controlling for*')\n",
    "\n",
    "[Note: At first glance the above formula looks different to the textbook but it is the same equation.]  \n",
    "[Intuition: $\\tilde{x}_j$ is the part of $x_j$ which cannot be accounted for by the other covariates; hence it is the residuals of regression of $x_j$ on the other covariates.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goodness-of-fit: \n",
    "### Same as for simple regression model\n",
    "\n",
    "- **SST** = Total Sum of Squares\n",
    "- **SSE** = Explained Sum of Squares\n",
    "- **SSR** = Residual Sum of Squares\n",
    "\n",
    "$$ SST = \\sum_{i=1}^n (y_i-\\bar{y})^2 \\quad SSE = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2 \\quad SSR = \\sum_{i=1}^n (\\hat{u}_i)^2 $$\n",
    "\n",
    "$$ SST=SSE + SSR$$\n",
    "\n",
    "$$ R^2=\\frac{SSE}{SST}=1-\\frac{SSR}{SST}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### R-squared\n",
    "\n",
    "\n",
    "The R-squared never decreases, and usually increases, when another independent variable is added to a regression.\n",
    "\n",
    "This is because the SSR can never increase when you add more regressors to the model (why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Simple Regression:\n",
    "<center><img src='Material/Topic3_Fig1.png' width='620' height='80'/></center>\n",
    "Multiple Regression:\n",
    "<center><img src='Material/Topic3_Fig2.png' width='600' height='80'/></center>\n",
    "(Based on: WAGE1.dta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From World 2 to World 1\n",
    "\n",
    "The discussion above was about properties of the OLS estimator that hold in any dataset (world 2).\n",
    "\n",
    "Now we discuss under which circumstances OLS estimates tell us something about the DGP (world 1).\n",
    "\n",
    "Remember, if the expected value of the OLS estimator is equal the true parameter (from world 1) our OLS estimator is unbiased ($E[\\hat{\\beta}_j]=\\beta_j$)\n",
    "\n",
    "<center><img src='Material/Topic3_Fig3.png' width='200' height='40'/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From World 2 to World 1 (cont.)\n",
    "\n",
    "The multiple linear regression (MLR) assumptions are mostly straightforward extensions of those we saw for the simple regression model.\n",
    "\n",
    "MLR 1. – 4. describe the conditions under which OLS gives us unbiased estimates of the underlying population model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assumptions\n",
    "\n",
    "Assumption MLR.1: **Linear in parameters**: \n",
    "$$y=\\beta_0+\\beta_1 X_1 + \\beta_2 X_2 + ...+ \\beta_k X_k + u$$\n",
    "\n",
    "Assumption MLR.2: **Random sampling**:  \n",
    "We have a random sample of n observations of $\\{(x_{i1}, x_{i2},...,x_{ik},y_i): \\, i=1,2,...,n\\}$\n",
    "\n",
    "Assumption MLR.3: **No perfect collinearity**:  \n",
    "In the sample, none of the independent variables is constant and there are no exact linear relationships among the independent variables.\n",
    "\n",
    "Assumption MLR.4: **Zero conditional mean**:  \n",
    "The error u has an expected value of zero given any values of the independent variables: \n",
    "$E[u|x_1, x_2,…,x_k]=0$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MLR. 1-3\n",
    "\n",
    "We will talk about **MLR. 1** later.\n",
    "\n",
    "Assumption **MLR. 2** is straightforward.\n",
    "\n",
    "Assumption **MLR.3** is new: *No perfect collinearity*.  \n",
    "Key in practice: No exact *linear dependence* between independent variables.  \n",
    "If there is linear dependence between variables, then we say there is perfect collinearity. In such a case we cannot estimate the parameters using OLS.  \n",
    "Examples:  \n",
    "$x_2 = 4*x_1$  \n",
    "$x_3 = 5*x_1 + 7*x_2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example of perfect collinearity\n",
    "\n",
    "- This type of model can’t be estimated by OLS:\n",
    "$$ cont = \\beta_0 + \\beta_1 IncomeInDollars + \\beta_2 IncomeInThousandsOfDollars + u$$\n",
    "since $IncomeInThousandsOfDollars=1000*IncomeInDollars$ there is linear dependence.\n",
    "\n",
    "- However, this type of model can be estimated by OLS:\n",
    "$$ cont = \\beta_0 + \\beta_1 Income+ \\beta_2 Income^2 + u$$\n",
    "while $Income$ and $Income^2$ are related, they are *not* **linearly** related.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLR.4: Zero conditional mean\n",
    "\n",
    "MLR.4 $E(u|x_1, x_2,...,x_k)=0$ is a direct extension of SLR.4. \n",
    "\n",
    "It is the most important of the four assumptions MLR.1-4, and implies that the error term u is **uncorrelated with all explanatory variables** in the population. \n",
    "\n",
    "When MLR.4 holds, we sometimes say that the explanatory variables are **exogenous**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why MLR.4 may fail\n",
    "\n",
    "MLR.4 may fail for the following reasons:\n",
    "- **Omitting an explanatory variable** that is correlated with any of the $x_1, x_2,...,x_k$.\n",
    "- **Misspecified functional relationship** between the dependent and independent variables. (We will talk more about this later)\n",
    "\n",
    "The first of these – omitted variables – is by far the biggest concern in empirical work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theorem 3.1:\n",
    "### Under MLR.1-4, OLS is unbiased\n",
    "$$ E[\\hat{\\beta}_j]=\\beta_j, \\quad j=1,2,...,k$$\n",
    "You do not have to know how to prove that OLS is unbiased for the multiple regression model. But you need to know:\n",
    "- The definition above and what it means\n",
    "- The assumptions you need for unbiasedeness (MLR.1-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Towards a weaker version of MLR 4\n",
    "<center><img src='Material/Topic3_Fig4.png' width='300' height='30'/></center>\n",
    "\n",
    "- What is the effect of attending a private university?\n",
    "\n",
    "- Motivation: private selective university (e.g. Harvard, Yale) are more expensive compared to public universities (e.g., University of Texas). Do they lead to higher earnings later in life?\n",
    "\n",
    "- Problem: students who go to private selective universities are different from those who go to public colleges (selection bias).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Return to Private University\n",
    "\n",
    "- Dale and Kruger (2002) have data on students who graduated in 1972 including earnings later in life.\n",
    "\n",
    "- This data also includes information on which institutions the student applied to and at which institutions they were accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider the following model:\n",
    "$$wage = \\beta_0 + \\beta_1 private + \\gamma_1 applied private + \\gamma_2 accepted private + \\gamma_3 SAT score+ u$$\n",
    "where  \n",
    "$private=1$ if went to private university, 0 otherwise.  \n",
    "$appliedprivate=1$ if applied to private university, 0 otherwise.  \n",
    "$acceptedprivate=1$ is accepted at private university, 0 otherwise.  \n",
    "$SAT score=$ students score on SAT test\n",
    "\n",
    "- Do people who applied for and got accepted to the same colleges, with the same SAT score, differ systematically in other factors that affect wage?  \n",
    "Is:  \n",
    "$E[u|private,appliedprivate,acceptedprivate,SATscore]=E[appliedprivate,acceptedprivate,SATscore]$  \n",
    "Discuss.\n",
    "\n",
    "[Note: SAT was a common test for university access at the time.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='Material/Topic3_Fig5.png' width='650' height='100'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weaker version of MLR 4\n",
    "\n",
    "Consider the following model:\n",
    "$$y=\\beta_0 + \\beta_1 P + \\gamma_1 x_1 + \\gamma_2 x_2 +... \\gamma_k x_k + u$$\n",
    "where:  \n",
    "$P=$ variable of interest.  \n",
    "$\\beta_1=$ parameter of interest.  \n",
    "$x_1,x_2,...,x_k=$ variables *not* of interest (control variables).  \n",
    "$\\gamma_1, \\gamma_2,..., \\gamma_k=$ parameters *not* of interest.\n",
    "\n",
    "OLS leads to unbiased estimates of causal effect of $\\beta_1$ (but *not* $\\gamma_1, \\gamma_2,..., \\gamma_k$) if:\n",
    "$$E[u|P,x_1,...,x_k]=E[u|x_1,...,x_k]$$\n",
    "\n",
    "*In words*: if the average value of the error term $u$ in the population, when holding constant $\\gamma_1, \\gamma_2,..., \\gamma_k$ is the same for all values of $P$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weaker MLR 4: intuition\n",
    "\n",
    "$$y=\\beta_0 + \\beta_1 P + \\gamma_1 x_1 + \\gamma_2 x_2 +... \\gamma_k x_k + u$$\n",
    "\n",
    "Ask yourself:  \n",
    "Holding *observed* other factors ($\\gamma_1, \\gamma_2,..., \\gamma_k$) constant, **is P unrelated to the unobserved other factors**?  \n",
    "  - If yes, OLS estimator of the effect of variable of interest $\\beta_1$ is unbiased (i.e. $\\hat{\\beta}_1$ is unbiased)  \n",
    "  - If no, OLS estimator of the effect of variable of interest $\\beta_1$ is biased ($\\hat{\\beta}_1$ is biased)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 2: Gender Wage Gap\n",
    "\n",
    "$$wage = \\beta_0+\\beta_1 female +\\gamma_1 educ + \\gamma_2 exper + \\gamma_3 tenure + u$$\n",
    "\n",
    "We are interested in $\\beta_1$, which shows the causal effect of being female on wage.\n",
    "\n",
    "We **control for** education, experience and tenure to make the comparison ceteris paribus. We do not care about their causal effect on wage. \n",
    "\n",
    "Is $E[u|female, educ, exper, tenure]=E[u|educ, exper, tenure]$?\n",
    "\n",
    "Or do women *with the same years of education, experience and tenure* differ systematically in unobserved factors that matter for wage ($u$) from men?  Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='Material/Topic3_Fig6.png' width='600' height='80'/></center>\n",
    "\n",
    "Interpretation: women earn on average $2.51 less per hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='Material/Topic3_Fig7.png' width='600' height='120'/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gender wage gap\n",
    "\n",
    "Even if \n",
    "$$E[u|female, educ, exper, tenure] \\color{red}{\\neq} E[u|educ, exper, tenure]$$\n",
    "\n",
    "$\\hat{\\beta}_1$ is still interesting!\n",
    "\n",
    "$\\hat{\\beta}_1$ shows us the average wage difference between men and women (in our sample) who have the same level of education, experience and tenure. \n",
    "\n",
    "[Just that we cannot interpret it as causation.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Omitted Variable Bias (OVB)\n",
    "\n",
    "One main reason why MLR 4. fails is if we omit an important variable in the regression $\\to$ OLS estimates will be biased.  \n",
    "(Note: We often still find the relationship in the data interesting (world 2)).\n",
    "\n",
    "We might be able to deduce in which direction the OLS is biased and thus learn if our OLS estimate is likely larger or smaller than the true effect.\n",
    "\n",
    "We can do this by thinking carefully how the OLS estimates would look like if we were to include the omitted variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OVB: \n",
    "\n",
    "<center><img src='Material/Topic3_Fig8.png' width='600' height='80'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OVB: Simple case\n",
    "\n",
    "- Let’s first look what happens mechanically (in world 2) if we omit a variable.\n",
    "\n",
    "- Suppose we are interested in the effect of education on wage.\n",
    "\n",
    "- How would the regression line look like if we could partial out the effect of ability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OVB formula: simple case\n",
    "\n",
    "- Simple regression: $\\tilde{y}=\\tilde{\\beta}_0 +\\tilde{\\beta}_1 x_1$\n",
    "- Multiple regression: $\\hat{y}=\\hat{\\beta}_0 +\\hat{\\beta}_1 x_1 +\\hat{\\beta}_2 x_2$\n",
    "\n",
    "We know that the simple regression coefficient will be:\n",
    "$$\\tilde{\\beta}_1=\\hat{\\beta}_1 + \\hat{\\beta}_2 \\tilde{\\delta}_1$$\n",
    "\n",
    "where $\\tilde{\\delta}_1$ is the slope coefficient from a simple regression of $x_2$ on $x_1$. How can this be understood?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Simple regression: $\\tilde{y}=\\tilde{\\beta}_0 +\\tilde{\\beta}_1 x_1$\n",
    "- Multiple regression: $\\hat{y}=\\hat{\\beta}_0 +\\hat{\\beta}_1 x_1 +\\hat{\\beta}_2 x_2$\n",
    "$$\\tilde{\\beta}_1=\\hat{\\beta}_1 + \\hat{\\beta}_2 \\tilde{\\delta}_1$$\n",
    "\n",
    "So, the estimates of $\\beta_1$, namely $\\hat{\\beta}_1$ and $\\tilde{\\beta}_1$ are the same if the second term, $\\hat{\\beta}_2 \\tilde{\\delta}_1$ is zero; i.e., if...\n",
    "- The partial effect of $x_2$ on $\\hat{y}$ is **zero** in the sample ($\\hat{\\beta}_2=0$).  \n",
    "$ and/or $\n",
    "- $x_1$ and $x_2$ **are uncorrelated** in the sample ($\\tilde{\\delta}_1=0$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OVB: simple case\n",
    "\n",
    "The formula $\\tilde{\\beta}_1=\\hat{\\beta}_1 + \\hat{\\beta}_2 \\tilde{\\delta}_1$ holds always in the data (World 2).\n",
    "\n",
    "In the special case where $\\hat{\\beta_1}$ and $\\hat{\\beta_2}$ are unbiased (i.e., MLR 1. – 4. would hold if we could include/observe $x_1$ and $x_2$), the bias is $\\tilde{\\beta}_1$ is given by,\n",
    "$$ Bias(\\tilde{\\beta}_1)=\\beta_2 \\delta_1$$\n",
    "\n",
    "Note: define $Bias(\\hat{\\beta})=E[\\hat{\\beta}]-\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Signing Bias\n",
    "\n",
    "<center><img src='Material/Topic3_Fig9.png' width='600' height='80'/></center>\n",
    "\n",
    "Note: this shows only the bias in the case where a regression with $x_1$ and $x_2$ would have been unbiased. However, the table can still be used to learn how $\\hat{\\beta}_1$ and $\\tilde{\\beta}_1$ differ (in world 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Omitted variable bias: More general cases\n",
    "\n",
    "- Deriving the sign of omitted variable bias when there are multiple regressors in the estimated model is more difficult.\n",
    "- In general, correlation between a single explanatory variable and the error results in all estimates being biased. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Homoskedasticity (MLR 5)\n",
    "\n",
    "Assumption MLR.5: **Homoskedasticity**.  \n",
    "The error $u$ has the same variance given any value of the explanatory variables: \n",
    "\n",
    "$$Var(u|x_1,x_2,...,x_k)=\\sigma^2$$\n",
    "\n",
    "This means that the variance in the error term $u$ conditional on the explanatory variables, is the **same** for all values of the explanatory variables.\n",
    "\n",
    "If this is not the case, there is **heteroskedasticity** and the formula for the variance of $\\beta_j$ has to be adjusted. We will talk about this later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variance of the OLS estimator\n",
    "\n",
    "Under assumptions MLR.1-5 (also known as the Gauss-Markov assumptions): \n",
    "<center><img src='Material/Topic3_Fig10.png' width='300' height='80'/></center>\n",
    "for $j=1,2,...,k$, where  \n",
    "$SST_j=\\sum_{i=1}^n (x_{ij}-\\bar{x}_j)^2$ is the total sum of squares of $x_j$  \n",
    "$R_j^2$ is the R-squared from regressing $x_j$ on all other regressors (and including an intercept)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpreting the variance formula\n",
    "<center><img src='Material/Topic3_Fig10.png' width='300' height='80'/></center>\n",
    "\n",
    "The variance of the estimator is **high** (which is typically *un*desirable) if...\n",
    "- The variance of the error term is high.\n",
    "- The total sampling variance of the $x_j$ is low (e.g. due to low variance or small sample).\n",
    "- The $R_j^2$ is high. Note that, as $R_j^2$ gets close to 1 – due to near linear dependence amongst regressors (multicollinearity) – the variance can become very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimator of Variance of $\\hat{\\beta}_j$\n",
    "\n",
    "We already saw that\n",
    "$$Var(\\hat{\\beta}_j) = \\frac{\\sigma^2}{SST_j (1-R_j^2)}$$\n",
    "but we don't observe $\\sigma^2$, so we cannot just use this.\n",
    "\n",
    "To get an estimator of $Var(\\hat{\\beta}_j)$ we replace the true (and unobserved) parameter $\\sigma^2$ with $\\hat{\\sigma}^2$, which is an estimator of $\\sigma^2$.\n",
    "    $$ \\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n \\hat{u}_i^2}{n-k-1} = \\frac{SSR}{n-k-1}$$\n",
    "and the resulting estimator is\n",
    "$$\\widehat{Var}(\\hat{\\beta}_j)=\\frac{\\hat{\\sigma}^2}{SST_j (1-R_j^2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimating standard errors of the OLS estimates\n",
    "\n",
    "The main practical usage of the variance formula is for **calculating standard errors** of the OLS estimates. **Standard errors are estimates of the standard deviation of $\\hat{\\beta}_ j$**\n",
    "\n",
    "Recall that the standard deviation (sd) is equal to the square root of the variance.\n",
    "$$ sd(\\hat{\\beta}_j) = \\sqrt{Var(\\hat{\\beta}_j)} $$\n",
    "\n",
    "The standard error is simply the square root of the estimator of the variance of .\n",
    "$$ se(\\hat{\\beta}_j) = \\sqrt{\\widehat{Var}(\\hat{\\beta}_j)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Standard error (cont.)\n",
    "\n",
    "$$ se(\\hat{\\beta}_j) = \\sqrt{\\widehat{Var}(\\hat{\\beta}_j)} $$\n",
    "\n",
    "$$ se(\\hat{\\beta}_j) = \\sqrt{\\frac{\\hat{\\sigma}^2}{SST_j (1-R_j^2)}}$$\n",
    "\n",
    "The standard error is large (which is typically *un*desirable) if...\n",
    "- SSR is high\n",
    "- The total sampling variance of the $x_j$ is low (e.g. due to low variance or small sample).\n",
    "- The $R_j^2$ is high. \n",
    "\n",
    "We use standard errors for **hypothesis testing** – more on this in topic 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Standard errors in Stata\n",
    "\n",
    "<center><img src='Material/Topic3_Fig11.png' width='600' height='80'/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Efficiency of OLS: The Gauss-Markov Theorem\n",
    "\n",
    "- Under assumptions MLR.1-5, OLS is the **Best Linear Unbiased Estimator** (BLUE) of the population parameters.\n",
    "- Best = smallest variance\n",
    "- It’s reassuring to know that, under MLR.1-5, you cannot find a better estimator than OLS. \n",
    "- If one or several of these assumptions fail, OLS is no longer BLUE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review/Other"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
